{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "259174ea-1b58-407c-a0ad-15a8ce7e7688",
   "metadata": {},
   "source": [
    "## Module 5: Analyse, diagnose and improve a modelâ€‹\n",
    "\n",
    "In the excercise of this week you will be working with financial data in order to (hopefully) find a portfolio of equities which outperform SP500. The data that you are gonna work with has two main sources: \n",
    "* Financial data from the companies extracted from the quarterly company reports (mostly extracted from [macrotrends](https://www.macrotrends.net/) so you can use this website to understand better the data and get insights on the features, for example [this](https://www.macrotrends.net/stocks/charts/AAPL/apple/revenue) is the one corresponding to APPLE)\n",
    "* Stock prices, mostly extracted from [morningstar](https://indexes.morningstar.com/page/morningstar-indexes-empowering-investor-success?utm_source=google&utm_medium=cpc&utm_campaign=MORNI%3AG%3ASearch%3ABrand%3ACore%3AUK%20MORNI%3ABrand%3ACore%3ABroad&utm_content=engine%3Agoogle%7Ccampaignid%3A18471962329%7Cadid%3A625249340069&utm_term=morningstar%20index&gclid=CjwKCAjws9ipBhB1EiwAccEi1Fu6i20XHVcxFxuSEtJGF0If-kq5-uKnZ3rov3eRkXXFfI5j8QBtBBoCayEQAvD_BwE), which basically tell us how the stock price is evolving so we can use it both as past features and the target to predict).\n",
    "\n",
    "Before going to the problem that we want to solve, let's comment some of the columns of the dataset:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1d8b14-f545-4729-a4d4-9d500910a705",
   "metadata": {},
   "source": [
    "* `Ticker`: a [short name](https://en.wikipedia.org/wiki/Ticker_symbol) to identify the equity (that you can use to search in macrotrends)\n",
    "* `date`: the date of the company report (normally we are gonna have 1 every quarter). This is for informative purposes but you can ignore it when modeling.\n",
    "* `execution date`: the date when we would had executed the algorithm for that equity. We want to execute the algorithm once per quarter to create the portfolio, but the release `date`s of all the different company reports don't always match for the quarter, so we just take a common `execution_date` for all of them.\n",
    "* `stock_change_div_365`: what is the % change of the stock price (with dividens) in the FOLLOWING year after `execution date`. \n",
    "* `sp500_change_365`: what is the % change of the SP500 in the FOLLOWING year after `execution date`.\n",
    "* `close_0`: what is the price at the moment of `execution date`\n",
    "* `stock_change__minus_120` what is the % change of the stock price in the last 120 days\n",
    "* `stock_change__minus_730`: what is the % change of the stock price in the last 730 days\n",
    "\n",
    "The rest of the features can be divided beteween financial features (the ones coming from the reports) and technical features (coming from the stock price). We leave the technical features here as a reference: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600b5225-4f40-4d8d-ac44-835fd9909982",
   "metadata": {},
   "outputs": [],
   "source": [
    "technical_features = ['close_0', 'close_sp500_0', 'close_365', 'close_sp500_365',\n",
    "       'close__minus_120', 'close_sp500__minus_120', 'close__minus_365',\n",
    "       'close_sp500__minus_365', 'close__minus_730', 'close_sp500__minus_730',\n",
    "       'stock_change_365','stock_change_div_365', 'sp500_change_365', 'stock_change__minus_120',\n",
    "       'sp500_change__minus_120', 'stock_change__minus_365',\n",
    "       'sp500_change__minus_365', 'stock_change__minus_730','sp500_change__minus_730',\n",
    "       'std__minus_365','std__minus_730','std__minus_120']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc137a7-3a45-4e05-9d3d-8bf047e23691",
   "metadata": {},
   "source": [
    "The problem that we want to solve is basically find a portfolio of `top_n` tickers (initially set to 10) to invest every `execution date` (basically once per quarter) and the goal is to have a better return than `SP500` in the following year. The initial way to model this is to have a binary target which is 1 when `stock_change_div_365` - `sp500_change_365` (the difference between the return of the equity and the SP500 in the following year) is positive or 0 otherwise. So we try to predict the probability of an equity of improving SP500 in the following year, we take the `top_n` equities and compute their final return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb508f7e-8380-44d4-835d-bd3a67f4e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from plotnine import ggplot, geom_histogram, aes, geom_col, coord_flip,geom_bar,scale_x_discrete, geom_point, theme,element_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b7419-170a-4788-a00a-8201276a85cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of trees in lightgbm\n",
    "n_trees = 40\n",
    "minimum_number_of_tickers = 1500\n",
    "# Number of the quarters in the past to train\n",
    "n_train_quarters = 36\n",
    "# number of tickers to make the portfolio\n",
    "top_n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac496b3a-bdf9-4ef6-949e-2d09d9b00063",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_feather(\"data/financials_against_return.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd85db3-f0d4-449c-bf55-1f31fb48853e",
   "metadata": {},
   "source": [
    "Remove these quarters which have les than `minimum_number_of_tickers` tickers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cd0de7-03fd-4e95-91ef-ed651c79d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quarter_lengths = data_set.groupby([\"execution_date\"]).size().reset_index().rename(columns = {0:\"count\"})\n",
    "data_set = pd.merge(data_set, df_quarter_lengths, on = [\"execution_date\"])\n",
    "data_set = data_set[data_set[\"count\"]>=minimum_number_of_tickers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd82a630-bff7-410a-b1d0-a2e5b86fa15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951ff1b-db7d-4667-bcb6-2f0f41180262",
   "metadata": {},
   "source": [
    "Create the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59914129-455f-4853-90a0-37065fc28704",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set[\"diff_ch_sp500\"] = data_set[\"stock_change_div_365\"] - data_set[\"sp500_change_365\"]\n",
    "\n",
    "data_set.loc[data_set[\"diff_ch_sp500\"]>0,\"target\"] = 1\n",
    "data_set.loc[data_set[\"diff_ch_sp500\"]<0,\"target\"] = 0\n",
    "\n",
    "data_set[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505261e7-8d7d-481b-9317-06472b02786a",
   "metadata": {},
   "source": [
    "This function computes the main metric that we want to optimize: given a prediction where we have probabilities for each equity, we sort the equities in descending order of probability, we pick the `top_n` ones, and we we weight the returned `diff_ch_sp500` by the probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3af4a-fc9b-43cf-b105-024e60dc25a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_performance_of_stocks(df,metric):\n",
    "    df[\"norm_prob\"] = 1/len(df)\n",
    "    return np.sum(df[\"norm_prob\"]*df[metric])\n",
    "\n",
    "def get_top_tickers_per_prob(preds):\n",
    "    if len(preds) == len(train_set):\n",
    "        data_set = train_set.copy()\n",
    "    elif len(preds) == len(test_set):\n",
    "        data_set = test_set.copy()\n",
    "    else:\n",
    "        assert (\"Not matching train/test\")\n",
    "    data_set[\"prob\"] = preds\n",
    "    data_set = data_set.sort_values([\"prob\"], ascending = False)\n",
    "    data_set = data_set.head(top_n)\n",
    "    return data_set\n",
    "\n",
    "# main metric to evaluate: average diff_ch_sp500 of the top_n stocks\n",
    "def top_wt_performance(preds, train_data):\n",
    "    top_dataset = get_top_tickers_per_prob(preds)\n",
    "    return \"weighted-return\", get_weighted_performance_of_stocks(top_dataset,\"diff_ch_sp500\"), True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa6f3a4-b39f-461f-bb2e-7056c8505f89",
   "metadata": {},
   "source": [
    "We have created for you a function to make the `train` and `test` split based on a `execution_date`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d4f6a1-276e-441a-8222-39ce09f5bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_by_period(data_set, test_execution_date,include_nulls_in_test = False):\n",
    "    # we train with everything happening at least one year before the test execution date\n",
    "    train_set = data_set.loc[data_set[\"execution_date\"] <= pd.to_datetime(test_execution_date) - pd.Timedelta(350, unit = \"day\")]\n",
    "    # remove those rows where the target is null\n",
    "    train_set = train_set[~pd.isna(train_set[\"diff_ch_sp500\"])]\n",
    "    execution_dates = train_set.sort_values(\"execution_date\")[\"execution_date\"].unique()\n",
    "    # Pick only the last n_train_quarters\n",
    "    if n_train_quarters!=None:\n",
    "        train_set = train_set[train_set[\"execution_date\"].isin(execution_dates[-n_train_quarters:])]\n",
    "        \n",
    "    # the test set are the rows happening in the execution date with the concrete frequency\n",
    "    test_set = data_set.loc[(data_set[\"execution_date\"] == test_execution_date)]\n",
    "    if not include_nulls_in_test:\n",
    "        test_set = test_set[~pd.isna(test_set[\"diff_ch_sp500\"])]\n",
    "    test_set = test_set.sort_values('date', ascending = False).drop_duplicates('Ticker', keep = 'first')\n",
    "    \n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffee41d-cd09-4c6f-a57a-32c98f739a8d",
   "metadata": {},
   "source": [
    "Ensure that we don't include features which are irrelevant or related to the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b39426-e296-4eab-9c5a-2be2c8fb3161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_to_remove():\n",
    "    columns_to_remove = [\n",
    "                         \"date\",\n",
    "                         \"improve_sp500\",\n",
    "                         \"Ticker\",\n",
    "                         \"freq\",\n",
    "                         \"set\",\n",
    "                         \"close_sp500_365\",\n",
    "                         \"close_365\",\n",
    "                         \"stock_change_365\",\n",
    "                         \"sp500_change_365\",\n",
    "                         \"stock_change_div_365\",\n",
    "                         \"stock_change_730\",\n",
    "                         \"sp500_change_365\",\n",
    "                         \"stock_change_div_730\",\n",
    "                         \"diff_ch_sp500\",\n",
    "                         \"diff_ch_avg_500\",\n",
    "                         \"execution_date\",\"target\",\"index\",\"quarter\",\"std_730\",\"count\"]\n",
    "        \n",
    "    return columns_to_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42fb668-de10-4061-84d9-637f8bcf7342",
   "metadata": {},
   "source": [
    "This is the main modeling function, it receives a train test and a test set and trains a `lightgbm` in classification mode. We don't recommend to change the main algorithm for this excercise but we suggest to play with its hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d834a2fa-b10a-4d3b-bee3-0ee8fcf7adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def train_model(train_set,test_set,n_estimators = 300):\n",
    "\n",
    "    columns_to_remove = get_columns_to_remove()\n",
    "    \n",
    "    X_train = train_set.drop(columns = columns_to_remove, errors = \"ignore\")\n",
    "    X_test = test_set.drop(columns = columns_to_remove, errors = \"ignore\")\n",
    "    \n",
    "    \n",
    "    y_train = train_set[\"target\"]\n",
    "    y_test = test_set[\"target\"]\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_train,y_train)\n",
    "    lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "    \n",
    "    eval_result = {}\n",
    "    \n",
    " \n",
    "    objective = 'binary'\n",
    "    metric = 'binary_logloss' \n",
    "    params = {\n",
    "             \"random_state\":1, \n",
    "             \"verbosity\": -1,\n",
    "             \"n_jobs\":10, \n",
    "             \"n_estimators\":n_estimators,\n",
    "             \"objective\": objective,\n",
    "             \"metric\": metric}\n",
    "    \n",
    "    model = lgb.train(params = params,train_set = lgb_train,\n",
    "                      valid_sets = [lgb_test,lgb_train],\n",
    "                      feval = [top_wt_performance],\n",
    "                      callbacks = [lgb.record_evaluation(eval_result = eval_result)])\n",
    "    return model,eval_result,X_train,X_test\n",
    "\n",
    "\n",
    " \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80d097f-71de-45ba-9c5f-ae4916bf3282",
   "metadata": {},
   "source": [
    "This is the function which receives an `execution_date` and splits the dataset between train and test, trains the models and evaluates the model in test. It returns a dictionary with the different evaluation metrics in train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8b8338-458c-451b-9c26-3f861cdb8322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_for_execution_date(execution_date,all_results,all_predicted_tickers_list,all_models,n_estimators,include_nulls_in_test = False):\n",
    "        global train_set\n",
    "        global test_set\n",
    "        # split the dataset between train and test\n",
    "        train_set, test_set = split_train_test_by_period(data_set,execution_date,include_nulls_in_test = include_nulls_in_test)\n",
    "        train_size, _ = train_set.shape\n",
    "        test_size, _ = test_set.shape\n",
    "        model = None\n",
    "        X_train = None\n",
    "        X_test = None\n",
    "        \n",
    "        # if both train and test are not empty\n",
    "        if train_size > 0 and test_size>0:\n",
    "            model, evals_result, X_train, X_test = train_model(train_set,\n",
    "                                                              test_set,\n",
    "                                                              n_estimators = n_estimators)\n",
    "            \n",
    "            test_set['prob'] = model.predict(X_test)\n",
    "            predicted_tickers = test_set.sort_values('prob', ascending = False)\n",
    "            predicted_tickers[\"execution_date\"] = execution_date\n",
    "            all_results[(execution_date)] = evals_result\n",
    "            all_models[(execution_date)] = model\n",
    "            all_predicted_tickers_list.append(predicted_tickers)\n",
    "        return all_results,all_predicted_tickers_list,all_models,model,X_train,X_test\n",
    "\n",
    "\n",
    "execution_dates = np.sort( data_set['execution_date'].unique() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138e3ce-4495-4c30-b854-fc99bf62d2c0",
   "metadata": {},
   "source": [
    "This is the main training loop: it goes through each different `execution_date` and calls `run_model_for_execution_date`. All the results are stored in `all_results` and the predictions in `all_predicted_tickers_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e199bbfd-4ace-4a36-91d5-239e91981172",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "all_predicted_tickers_list = []\n",
    "all_models = {}\n",
    "\n",
    "for execution_date in execution_dates:\n",
    "    print(execution_date)\n",
    "    all_results,all_predicted_tickers_list,all_models,model,X_train,X_test = run_model_for_execution_date(execution_date,all_results,all_predicted_tickers_list,all_models,n_trees,False)\n",
    "all_predicted_tickers = pd.concat(all_predicted_tickers_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f60c1b8-4513-4f3c-8af9-f1f727753b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_results_into_df(set_):\n",
    "    df = pd.DataFrame()\n",
    "    for date in all_results:\n",
    "        df_tmp = pd.DataFrame(all_results[(date)][set_])\n",
    "        df_tmp[\"n_trees\"] = list(range(len(df_tmp)))\n",
    "        df_tmp[\"execution_date\"] = date\n",
    "        df= pd.concat([df,df_tmp])\n",
    "    \n",
    "    df[\"execution_date\"] = df[\"execution_date\"].astype(str)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2df0999-e477-4f33-8f20-7a05d553aace",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = parse_results_into_df(\"valid_0\")\n",
    "train_results = parse_results_into_df(\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc1a73b-dcde-4994-9530-3cd71aa7b064",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_final_tree = test_results.sort_values([\"execution_date\",\"n_trees\"]).drop_duplicates(\"execution_date\",keep = \"last\")\n",
    "train_results_final_tree = train_results.sort_values([\"execution_date\",\"n_trees\"]).drop_duplicates(\"execution_date\",keep = \"last\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb78ea49-3ce2-4761-8fb8-4f78536e4d5c",
   "metadata": {},
   "source": [
    "And this are the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f721c00-41de-48d1-bffc-fbe62594ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(test_results_final_tree) + geom_point(aes(x = \"execution_date\", y = \"weighted-return\")) + theme(axis_text_x = element_text(angle = 90, vjust = 0.5, hjust=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fd746b-25bb-4b92-8071-da9347f430eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(train_results_final_tree) + geom_point(aes(x = \"execution_date\", y = \"weighted-return\")) + theme(axis_text_x = element_text(angle = 90, vjust = 0.5, hjust=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47f8c79-1dab-49cd-81ee-15172f37b539",
   "metadata": {},
   "source": [
    "We have trained the first models for all the periods for you, but there are a lot of things which may be wrong or can be improved. Some ideas where you can start:\n",
    "* Try to see if there is any kind of data leakage or suspicious features\n",
    "* If the training part is very slow, try to see how you can modify it to execute faster tests\n",
    "* Try to understand if the algorithm is learning correctly\n",
    "* We are using a very high level metric to evaluate the algorithm so you maybe need to use some more low level ones\n",
    "* Try to see if there is overfitting\n",
    "* Try to see if there is a lot of noise between different trainings\n",
    "* To simplify, why if you only keep the first tickers in terms of Market Cap?\n",
    "* Change the number of quarters to train in the past"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc34a596-bef7-4686-95a8-ae37950b27b5",
   "metadata": {},
   "source": [
    "This function can be useful to compute the feature importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a809343f-63b7-4009-9179-4125132d2194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_feature_importance(model,top = 15):\n",
    "    fi = model.feature_importance()\n",
    "    fn = model.feature_name()\n",
    "    feature_importance = pd.DataFrame([{\"feature\":fn[i],\"imp\":fi[i]} for i in range(len(fi))])\n",
    "    feature_importance = feature_importance.sort_values(\"imp\",ascending = False).head(top)\n",
    "    feature_importance = feature_importance.sort_values(\"imp\",ascending = True)\n",
    "    plot = ggplot(feature_importance,aes(x = \"feature\",y  = \"imp\")) + geom_col(fill = \"lightblue\") + coord_flip() +  scale_x_discrete(limits = feature_importance[\"feature\"])\n",
    "    return plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae276d6-44d7-47de-a214-88acae05bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import lognorm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c349a1-c43e-41ed-a339-2b40921b8815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88eda069",
   "metadata": {},
   "source": [
    "Mirar si el modelo aprende, oevrftinign o no, feature importance y data leakage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zrive-ds-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
